{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for training a denoiser\n",
    "import os\n",
    "\n",
    "os.environ['XLA_FLAGS']='--xla_gpu_cuda_data_dir=/gpfslocalsys/cuda/10.1.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import app\n",
    "from absl import flags\n",
    "import haiku as hk\n",
    "import jax\n",
    "#import optax\n",
    "from jax.experimental import optix\n",
    "import jax.numpy as jnp\n",
    "import numpy as onp\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "from flax.metrics import tensorboard\n",
    "\n",
    "# Import tensorflow for dataset creation and manipulation\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "#from jax_lensing.models.convdae import UResNet, SmallUResNet\n",
    "from jax_lensing.models.normalization import SNParamsTree as CustomSNParamsTree\n",
    "from jax_lensing.spectral import make_power_map\n",
    "from jax_lensing.utils import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import types\n",
    "from typing import Mapping, Optional, Sequence, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"kappatng\"\n",
    "output_dir = \"../weights/gp-sn1\"\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "training_steps = 45000\n",
    "train_split = \"90%\"\n",
    "noise_dist_std = 0.2\n",
    "spectral_norm = 1.\n",
    "gaussian_prior = True\n",
    "gaussian_path = \"../data/ktng/ktng_PS_theory.npy\"\n",
    "variant = \"EiffL\"\n",
    "model_name = \"SmallUResNet\"\n",
    "map_size = 360\n",
    "resolution = 0.29"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def forward(x, s, is_training=False):\n",
    "  if model_name == 'SmallUResNet':\n",
    "    denoiser = SmallUResNet(n_output_channels=1, variant=variant)\n",
    "  else:\n",
    "    raise NotImplementedError\n",
    "  return denoiser(x, s, is_training=is_training)\n",
    "\n",
    "model = hk.transform_with_state(forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import jax\n",
    "#import jax.numpy as jnp\n",
    "#import haiku as hk\n",
    "\n",
    "#import types\n",
    "from typing import Mapping, Optional, Sequence, Union\n",
    "\n",
    "def check_length(length, value, name):\n",
    "  if len(value) != length:\n",
    "    raise ValueError(f\"`{name}` must be of length 4 not {len(value)}\")\n",
    "\n",
    "class BlockV1(hk.Module):\n",
    "  \"\"\"ResNet V1 block with optional bottleneck.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      channels: int,\n",
    "      stride: Union[int, Sequence[int]],\n",
    "      use_projection: bool,\n",
    "      bn_config: Mapping[str, float],\n",
    "      bottleneck: bool,\n",
    "      transpose: bool = False,\n",
    "      name: Optional[str] = None\n",
    "  ):\n",
    "    super().__init__(name=name)\n",
    "    self.use_projection = use_projection\n",
    "\n",
    "    bn_config = dict(bn_config)\n",
    "    bn_config.setdefault(\"create_scale\", True)\n",
    "    bn_config.setdefault(\"create_offset\", True)\n",
    "    bn_config.setdefault(\"decay_rate\", 0.999)\n",
    "\n",
    "    if transpose:\n",
    "      maybe_transposed_conv = hk.Conv2DTranspose\n",
    "    else:\n",
    "      maybe_transposed_conv = hk.Conv2D\n",
    "\n",
    "    if self.use_projection:\n",
    "      self.proj_conv = maybe_transposed_conv(\n",
    "          output_channels=channels,\n",
    "          kernel_shape=1,\n",
    "          stride=stride,\n",
    "          with_bias=False,\n",
    "          padding=\"SAME\",\n",
    "          name=\"shortcut_conv\")\n",
    "\n",
    "      self.proj_batchnorm = hk.BatchNorm(name=\"shortcut_batchnorm\", **bn_config)\n",
    "\n",
    "    channel_div = 4 if bottleneck else 1\n",
    "    conv_0 = hk.Conv2D(\n",
    "        output_channels=channels // channel_div,\n",
    "        kernel_shape=1 if bottleneck else 3,\n",
    "        stride=1,\n",
    "        with_bias=False,\n",
    "        padding=\"SAME\",\n",
    "        name=\"conv_0\")\n",
    "    bn_0 = hk.BatchNorm(name=\"batchnorm_0\", **bn_config)\n",
    "\n",
    "    conv_1 = maybe_transposed_conv(\n",
    "        output_channels=channels // channel_div,\n",
    "        kernel_shape=3,\n",
    "        stride=stride,\n",
    "        with_bias=False,\n",
    "        padding=\"SAME\",\n",
    "        name=\"conv_1\")\n",
    "\n",
    "    bn_1 = hk.BatchNorm(name=\"batchnorm_1\", **bn_config)\n",
    "    layers = ((conv_0, bn_0), (conv_1, bn_1))\n",
    "\n",
    "    if bottleneck:\n",
    "      conv_2 = hk.Conv2D(\n",
    "          output_channels=channels,\n",
    "          kernel_shape=1,\n",
    "          stride=1,\n",
    "          with_bias=False,\n",
    "          padding=\"SAME\",\n",
    "          name=\"conv_2\")\n",
    "\n",
    "      bn_2 = hk.BatchNorm(name=\"batchnorm_2\", scale_init=jnp.zeros, **bn_config)\n",
    "      layers = layers + ((conv_2, bn_2),)\n",
    "\n",
    "    self.layers = layers\n",
    "\n",
    "  def __call__(self, inputs, is_training, test_local_stats):\n",
    "    out = shortcut = inputs\n",
    "\n",
    "    if self.use_projection:\n",
    "      shortcut = self.proj_conv(shortcut)\n",
    "      shortcut = self.proj_batchnorm(shortcut, is_training, test_local_stats)\n",
    "\n",
    "    for i, (conv_i, bn_i) in enumerate(self.layers):\n",
    "      out = conv_i(out)\n",
    "      out = bn_i(out, is_training, test_local_stats)\n",
    "      if i < len(self.layers) - 1:  # Don't apply relu on last layer\n",
    "        out = jax.nn.relu(out)\n",
    "\n",
    "    return jax.nn.relu(out + shortcut)\n",
    "\n",
    "class BlockGroup(hk.Module):\n",
    "  \"\"\"Higher level block for ResNet implementation.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      channels: int,\n",
    "      num_blocks: int,\n",
    "      stride: Union[int, Sequence[int]],\n",
    "      bn_config: Mapping[str, float],\n",
    "      bottleneck: bool,\n",
    "      use_projection: bool,\n",
    "      transpose: bool,\n",
    "      name: Optional[str] = None,\n",
    "  ):\n",
    "    super().__init__(name=name)\n",
    "\n",
    "    block_cls = BlockV1\n",
    "\n",
    "    self.blocks = []\n",
    "    for i in range(num_blocks):\n",
    "      self.blocks.append(\n",
    "          block_cls(channels=channels,\n",
    "                    stride=(1 if i else stride),\n",
    "                    use_projection=(i == 0 and use_projection),\n",
    "                    bottleneck=bottleneck,\n",
    "                    bn_config=bn_config,\n",
    "                    transpose=transpose,\n",
    "                    name=\"block_%d\" % (i)))\n",
    "\n",
    "  def __call__(self, inputs, is_training, test_local_stats):\n",
    "    out = inputs\n",
    "    for block in self.blocks:\n",
    "      out = block(out, is_training, test_local_stats)\n",
    "    return out\n",
    "\n",
    "\n",
    "class UResNet(hk.Module):\n",
    "  \"\"\" Implementation of a denoising auto-encoder based on a resnet architecture\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               blocks_per_group,\n",
    "               bn_config,\n",
    "               bottleneck,\n",
    "               channels_per_group,\n",
    "               use_projection,\n",
    "               name=None):\n",
    "    \"\"\"Constructs a Residual UNet model based on a traditional ResNet.\n",
    "    Args:\n",
    "      blocks_per_group: A sequence of length 4 that indicates the number of\n",
    "        blocks created in each group.\n",
    "      bn_config: A dictionary of two elements, ``decay_rate`` and ``eps`` to be\n",
    "        passed on to the :class:`~haiku.BatchNorm` layers. By default the\n",
    "        ``decay_rate`` is ``0.9`` and ``eps`` is ``1e-5``.\n",
    "      resnet_v2: Whether to use the v1 or v2 ResNet implementation. Defaults to\n",
    "        ``False``.\n",
    "      bottleneck: Whether the block should bottleneck or not. Defaults to\n",
    "        ``True``.\n",
    "      channels_per_group: A sequence of length 4 that indicates the number\n",
    "        of channels used for each block in each group.\n",
    "      use_projection: A sequence of length 4 that indicates whether each\n",
    "        residual block should use projection.\n",
    "      name: Name of the module.\n",
    "    \"\"\"\n",
    "    super().__init__(name=name)\n",
    "    self.resnet_v2 = False\n",
    "\n",
    "    bn_config = dict(bn_config or {})\n",
    "    bn_config.setdefault(\"decay_rate\", 0.9)\n",
    "    bn_config.setdefault(\"eps\", 1e-5)\n",
    "    bn_config.setdefault(\"create_scale\", True)\n",
    "    bn_config.setdefault(\"create_offset\", True)\n",
    "\n",
    "    # Number of blocks in each group for ResNet.\n",
    "    check_length(4, blocks_per_group, \"blocks_per_group\")\n",
    "    check_length(4, channels_per_group, \"channels_per_group\")\n",
    "\n",
    "    self.initial_conv = hk.Conv2D(\n",
    "        output_channels=32,\n",
    "        kernel_shape=7,\n",
    "        stride=2,\n",
    "        with_bias=False,\n",
    "        padding=\"SAME\",\n",
    "        name=\"initial_conv\")\n",
    "\n",
    "    if not self.resnet_v2:\n",
    "      self.initial_batchnorm = hk.BatchNorm(name=\"initial_batchnorm\",\n",
    "                                            **bn_config)\n",
    "\n",
    "    self.block_groups = []\n",
    "    self.up_block_groups = []\n",
    "    strides = (1, 2, 2, 1)\n",
    "    for i in range(4):\n",
    "      self.block_groups.append(\n",
    "          BlockGroup(channels=channels_per_group[i],\n",
    "                     num_blocks=blocks_per_group[i],\n",
    "                     stride=strides[i],\n",
    "                     bn_config=bn_config,\n",
    "                     bottleneck=bottleneck,\n",
    "                     use_projection=use_projection[i],\n",
    "                     transpose=False,\n",
    "                     name=\"block_group_%d\" % (i)))\n",
    "\n",
    "    for i in range(4):\n",
    "      self.up_block_groups.append(\n",
    "          BlockGroup(channels=channels_per_group[i],\n",
    "                     num_blocks=blocks_per_group[i],\n",
    "                     stride=strides[i],\n",
    "                     bn_config=bn_config,\n",
    "                     bottleneck=bottleneck,\n",
    "                     use_projection=use_projection[i],\n",
    "                     transpose=True,\n",
    "                     name=\"up_block_group_%d\" % (i)))\n",
    "\n",
    "    if self.resnet_v2:\n",
    "      self.final_batchnorm = hk.BatchNorm(name=\"final_batchnorm\", **bn_config)\n",
    "\n",
    "    self.final_upconv = hk.Conv2DTranspose(output_channels=1,\n",
    "                                kernel_shape=5,\n",
    "                                stride=2,\n",
    "                                padding=\"SAME\",\n",
    "                                name=\"final_upconv\")\n",
    "\n",
    "    self.final_conv = hk.Conv2DTranspose(output_channels=1,\n",
    "                                kernel_shape=5,\n",
    "                                stride=2,\n",
    "                                padding=\"SAME\",\n",
    "                                name=\"final_conv\")\n",
    "\n",
    "  def __call__(self, inputs, condition, is_training, test_local_stats=False):\n",
    "    out = inputs\n",
    "    out = jnp.concatenate([out, condition*jnp.ones_like(out)[...,[0]]], axis=-1)\n",
    "    out = self.initial_conv(out)\n",
    "\n",
    "    # Decreasing resolution\n",
    "    levels = []\n",
    "    for block_group in self.block_groups:\n",
    "      levels.append(out)\n",
    "      out = block_group(out, is_training, test_local_stats)\n",
    "\n",
    "    out = jnp.concatenate([out, condition*jnp.ones_like(out)],axis=-1)\n",
    "\n",
    "    # Increasing resolution\n",
    "    for i, block_group in enumerate(self.up_block_groups[::-1]):\n",
    "      out = block_group(out, is_training, test_local_stats)\n",
    "      out = jnp.concatenate([out, levels[-i-1]],axis=-1)\n",
    "\n",
    "    # Second to last upsampling, merging with input branch\n",
    "    return self.final_conv(out)/(jnp.abs(condition)*jnp.ones_like(inputs)+1e-3)\n",
    "\n",
    "class SmallUResNet(UResNet):\n",
    "  \"\"\"ResNet18.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               bn_config: Optional[Mapping[str, float]] = None,\n",
    "               name: Optional[str] = None):\n",
    "    \"\"\"Constructs a ResNet model.\n",
    "    Args:\n",
    "      bn_config: A dictionary of two elements, ``decay_rate`` and ``eps`` to be\n",
    "        passed on to the :class:`~haiku.BatchNorm` layers.\n",
    "      resnet_v2: Whether to use the v1 or v2 ResNet implementation. Defaults\n",
    "        to ``False``.\n",
    "      name: Name of the module.\n",
    "    \"\"\"\n",
    "    super().__init__(blocks_per_group=(2, 2, 2, 2),\n",
    "                     bn_config=bn_config,\n",
    "                     bottleneck=False,\n",
    "                     channels_per_group=(32, 64, 128, 128),\n",
    "                     use_projection=(True, True, True, True),\n",
    "                     name=name)\n",
    "\n",
    "class MediumUResNet(UResNet):\n",
    "  \"\"\"ResNet18.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               bn_config: Optional[Mapping[str, float]] = None,\n",
    "               name: Optional[str] = None):\n",
    "    \"\"\"Constructs a ResNet model.\n",
    "    Args:\n",
    "      bn_config: A dictionary of two elements, ``decay_rate`` and ``eps`` to be\n",
    "        passed on to the :class:`~haiku.BatchNorm` layers.\n",
    "      resnet_v2: Whether to use the v1 or v2 ResNet implementation. Defaults\n",
    "        to ``False``.\n",
    "      name: Name of the module.\n",
    "    \"\"\"\n",
    "    super().__init__(blocks_per_group=(2, 2, 2, 2),\n",
    "                     bn_config=bn_config,\n",
    "                     bottleneck=False,\n",
    "                     channels_per_group=(32, 64, 128, 128),\n",
    "                     use_projection=(True, True, True, True),\n",
    "                     name=name)\n",
    "\n",
    "class MediumUResNet(UResNet):\n",
    "  \"\"\"ResNet18.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               bn_config: Optional[Mapping[str, float]] = None,\n",
    "               name: Optional[str] = None):\n",
    "    \"\"\"Constructs a ResNet model.\n",
    "    Args:\n",
    "      bn_config: A dictionary of two elements, ``decay_rate`` and ``eps`` to be\n",
    "        passed on to the :class:`~haiku.BatchNorm` layers.\n",
    "      resnet_v2: Whether to use the v1 or v2 ResNet implementation. Defaults\n",
    "        to ``False``.\n",
    "      name: Name of the module.\n",
    "    \"\"\"\n",
    "    super().__init__(blocks_per_group=(2, 2, 2, 2),\n",
    "                     bn_config=bn_config,\n",
    "                     bottleneck=False,\n",
    "                     channels_per_group=(32, 64, 128, 128),\n",
    "                     use_projection=(True, True, True, True),\n",
    "                     name=name)\n",
    "\n",
    "def forward(x, s, is_training=False):\n",
    "    denoiser = MediumUResNet()\n",
    "    return denoiser(x, s, is_training=is_training)\n",
    "\n",
    "model = hk.transform_with_state(forward)\n",
    "\n",
    "#sn_fn = hk.transform_with_state(lambda x: hk.SNParamsTree(ignore_regex='[^?!.]*b$')(x))\n",
    "#sn_fn = hk.transform_with_state(lambda x: CustomSNParamsTree(ignore_regex='[^?!.]*b$',val=2.)(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class SmallUResNet(UResNet):\n",
    "  \"\"\"ResNet18.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               bn_config: Optional[Mapping[str, float]] = None,\n",
    "               use_bn: bool = True,\n",
    "               pad_crop: bool = False,\n",
    "               n_output_channels: int = 1,\n",
    "               variant: Optional[str] = 'EiffL',\n",
    "               name: Optional[str] = None):\n",
    "    \"\"\"Constructs a ResNet model.\n",
    "    Args:\n",
    "      bn_config: A dictionary of two elements, ``decay_rate`` and ``eps`` to be\n",
    "        passed on to the :class:`~haiku.BatchNorm` layers.\n",
    "      resnet_v2: Whether to use the v1 or v2 ResNet implementation. Defaults\n",
    "        to ``False``.\n",
    "      use_bn: Whether the network should use batch normalisation. Defaults to\n",
    "        ``True``.\n",
    "      n_output_channels: The number of output channels, for example to change in\n",
    "        the case of a complex denoising. Defaults to 1.\n",
    "      name: Name of the module.\n",
    "    \"\"\"\n",
    "    super().__init__(blocks_per_group=(2, 2, 2, 2),\n",
    "                     bn_config=bn_config,\n",
    "                     bottleneck=False,\n",
    "                     channels_per_group=(32, 64, 128, 128),\n",
    "                     use_projection=(True, True, True, True),\n",
    "                     # 320 -> 160 -> 80 -> 40\n",
    "                     # 360 -> 180 -> 90 -> 45\n",
    "                     strides=(1, 2, 2, 1),\n",
    "                     use_bn=use_bn,\n",
    "                     pad_crop=pad_crop,\n",
    "                     n_output_channels=n_output_channels,\n",
    "                     variant=variant,\n",
    "                     name=name)\n",
    "    \n",
    "def forward(x, s, is_training=False):\n",
    "    denoiser = SmallUResNet()\n",
    "    return denoiser(x, s, is_training=is_training)\n",
    "\n",
    "model = hk.transform_with_state(forward)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(step):\n",
    "  \"\"\"Linear scaling rule optimized for 90 epochs.\"\"\"\n",
    "  steps_per_epoch = 30000 // batch_size\n",
    "\n",
    "  current_epoch = step / steps_per_epoch  # type: float\n",
    "  lr = (1.0 * batch_size) / 32\n",
    "  boundaries = jnp.array((20, 40, 60)) * steps_per_epoch\n",
    "  values = jnp.array([1., 0.1, 0.01, 0.001]) * lr\n",
    "\n",
    "  index = jnp.sum(boundaries < step)\n",
    "  return jnp.take(values, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spectral_norm > 0:\n",
    "    sn_fn = hk.transform_with_state(\n",
    "        lambda x: CustomSNParamsTree(ignore_regex='[^?!.]*b$',\n",
    "                                     val=spectral_norm)(x)\n",
    "    )\n",
    "else:\n",
    "    sn_fn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "\"\"\"\n",
    "optimizer = optax.chain(\n",
    "  optax.adam(learning_rate=learning_rate),\n",
    "  #optax.scale_by_schedule(lr_schedule)\n",
    ")\n",
    "\"\"\"\n",
    "optimizer = optix.chain(\n",
    "    optix.adam(learning_rate=learning_rate),\n",
    "    optix.scale_by_schedule(lr_schedule)\n",
    ")\n",
    "\n",
    "rng_seq = hk.PRNGSequence(42)\n",
    "\n",
    "if gaussian_prior:\n",
    "    last_dim=2\n",
    "else:\n",
    "    last_dim=1\n",
    "\n",
    "\"\"\"\n",
    "params, state = model.init(next(rng_seq),\n",
    "                           jnp.zeros((1, map_size, map_size, last_dim)),\n",
    "                           jnp.zeros((1, 1, 1, 1)), is_training=True)\n",
    "\"\"\"\n",
    "params, state = model.init(next(rng_seq),\n",
    "                           jnp.zeros((1, map_size, map_size, last_dim)),\n",
    "                           jnp.zeros((1, 1, 1, 1)), is_training=True)\n",
    "\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sn_fn is not None:\n",
    "    _, sn_state = sn_fn.init(next(rng_seq), params)\n",
    "else:\n",
    "    sn_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gaussian_prior(map_data, sigma, ps_map):\n",
    "    data_ft = jnp.fft.fft2(map_data) / float(map_size)\n",
    "    return -0.5*jnp.sum(jnp.real(data_ft*jnp.conj(data_ft)) / (ps_map+sigma[0]**2))\n",
    "\n",
    "gaussian_prior_score = jax.vmap(jax.grad(log_gaussian_prior), in_axes=[0,0, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_size = jnp.pi * resolution / 180. / 60. #rad/pixel\n",
    "  # If the Gaussian prior is used, load the theoretical power spectrum\n",
    "if gaussian_prior:\n",
    "    ps_data = onp.load(gaussian_path).astype('float32')\n",
    "    ell = jnp.array(ps_data[0,:])\n",
    "    # massivenu: channel 4\n",
    "    ps_halofit = jnp.array(ps_data[1,:] / pixel_size**2) # normalisation by pixel size\n",
    "    # convert to pixel units of our simple power spectrum calculator\n",
    "    kell = ell / (360/3.5/0.5) / float(map_size)\n",
    "    # Interpolate the Power Spectrum in Fourier Space\n",
    "    power_map = jnp.array(make_power_map(ps_halofit, map_size, kps=kell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_fn(params, state, rng_key, batch, is_training=True):\n",
    "    if gaussian_prior:\n",
    "        # If requested, first compute the Gaussian prior\n",
    "        gaussian_score = gaussian_prior_score(batch['y'][...,0], batch['s'][...,0], power_map)\n",
    "        gaussian_score = jnp.expand_dims(gaussian_score, axis=-1)\n",
    "        net_input = jnp.concatenate([batch['y'], jnp.abs(batch['s'])**2 * gaussian_score],axis=-1)\n",
    "        res, state = model.apply(params, state, rng_key, net_input, batch['s'], is_training=is_training)\n",
    "    else:\n",
    "        res, state = model.apply(params, state, rng_key, batch['y'], batch['s'], is_training=is_training)\n",
    "        gaussian_score = jnp.zeros_like(res)\n",
    "    return batch, res, gaussian_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Training loss\n",
    "def loss_fn(params, state, rng_key, batch):\n",
    "    _, res, gaussian_score = score_fn(params, state, rng_key, batch)\n",
    "    loss = jnp.mean((batch['u'] + batch['s'] * (res + gaussian_score))**2)\n",
    "    return loss, state\n",
    "\"\"\"\n",
    "@jax.jit\n",
    "def loss_fn(params, state, rng_key, batch):\n",
    "    #res, state = model.apply(params, state, rng_key, batch['y'], batch['s'], is_training=True)\n",
    "    #loss = jnp.mean((batch['u'] + batch['s'] * res)**2)\n",
    "    _, res, gaussian_score = score_fn(params, state, rng_key, batch)\n",
    "    loss = jnp.mean((batch['u'] + batch['s'] * (res + gaussian_score))**2)\n",
    "    \n",
    "    return loss, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@jax.jit\n",
    "def update(params, state, sn_state, rng_key, opt_state, batch):\n",
    "    (loss, state), grads = jax.value_and_grad(loss_fn, has_aux=True)(params, state, rng_key, batch)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    if spectral_norm > 0:\n",
    "        new_params, new_sn_state = sn_fn.apply(None, sn_state, None, new_params)\n",
    "    else:\n",
    "        new_sn_state = sn_state\n",
    "    return loss, new_params, state, new_sn_state, new_opt_state\n",
    "\"\"\"\n",
    "@jax.jit\n",
    "def update(params, state, sn_state, rng_key, opt_state, batch):\n",
    "    (loss, state), grads = jax.value_and_grad(loss_fn, has_aux=True)(params, state, rng_key, batch)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optix.apply_updates(params, updates)\n",
    "    if spectral_norm > 0:\n",
    "        new_params, new_sn_state = sn_fn.apply(None, sn_state, None, new_params)\n",
    "    else:\n",
    "        new_sn_state = sn_state\n",
    "    \n",
    "    return loss, new_params, state, new_sn_state, new_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(dataset, batch_size, map_size, noise_dist_std, train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dm-haiku                          0.0.2\n",
      "jax                               0.1.76\n",
      "jax-lensing                       0.1                   /gpfsdswork/projects/rech/xdy/utb76xl/jax-lensing\n",
      "jaxlib                            0.1.59+cuda101\n"
     ]
    }
   ],
   "source": [
    "#imshow(next(train)['y'][0,...,0]); colorbar()\n",
    "!pip list |grep dm-haiku\n",
    "!pip list |grep jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1f673104ad41af83937710cdea0e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=45000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.24533342\n",
      "100 0.15117913\n",
      "200 0.12728429\n",
      "300 0.11957041\n",
      "400 0.096686974\n",
      "500 0.13048634\n",
      "600 0.12288369\n",
      "700 0.14564525\n",
      "800 0.17127897\n",
      "900 0.041527882\n",
      "1000 0.1104981\n",
      "1100 0.15135624\n",
      "1200 0.09203073\n",
      "1300 0.08488464\n",
      "1400 0.06366562\n",
      "1500 0.16739926\n",
      "1600 0.09342241\n",
      "1700 0.05578177\n",
      "1800 0.13369097\n",
      "1900 0.1356984\n",
      "2000 0.07488945\n",
      "2100 0.09273893\n",
      "2200 0.056852736\n",
      "2300 0.08580434\n",
      "2400 0.13084453\n",
      "2500 0.068610445\n",
      "2600 0.07375628\n",
      "2700 0.1866941\n",
      "2800 0.15078354\n",
      "2900 0.12948023\n",
      "3000 0.099533066\n",
      "3100 0.15440026\n",
      "3200 0.14018609\n",
      "3300 0.16526112\n",
      "3400 0.10145087\n",
      "3500 0.061057575\n",
      "3600 0.17772189\n",
      "3700 0.09544975\n",
      "3800 0.08907546\n",
      "3900 0.1365962\n",
      "4000 0.1452686\n",
      "4100 0.06310775\n",
      "4200 0.099881336\n",
      "4300 0.15137736\n",
      "4400 0.10842813\n",
      "4500 0.09851127\n",
      "4600 0.105947345\n",
      "4700 0.035493676\n",
      "4800 0.086015806\n",
      "4900 0.083303005\n",
      "5000 0.057356726\n",
      "5100 0.06774258\n",
      "5200 0.083397195\n",
      "5300 0.08902822\n",
      "5400 0.055791702\n",
      "5500 0.0654502\n",
      "5600 0.112350084\n",
      "5700 0.09977041\n",
      "5800 0.044886384\n",
      "5900 0.037235193\n",
      "6000 0.06973509\n",
      "6100 0.080864124\n",
      "6200 0.0813794\n",
      "6300 0.087730065\n",
      "6400 0.08226264\n",
      "6500 0.09117964\n",
      "6600 0.05507596\n",
      "6700 0.09566277\n",
      "6800 0.044921845\n",
      "6900 0.09857264\n",
      "7000 0.08561301\n",
      "7100 0.0466232\n",
      "7200 0.09043235\n",
      "7300 0.06375611\n",
      "7400 0.100375056\n",
      "7500 0.038439006\n",
      "7600 0.06534257\n",
      "7700 0.0609496\n",
      "7800 0.06060149\n",
      "7900 0.07119335\n",
      "8000 0.082427636\n",
      "8100 0.13486949\n",
      "8200 0.16168992\n",
      "8300 0.060977653\n",
      "8400 0.085378096\n",
      "8500 0.10623166\n",
      "8600 0.13167122\n",
      "8700 0.0902185\n",
      "8800 0.099124074\n",
      "8900 0.07053732\n",
      "9000 0.066488765\n",
      "9100 0.046699066\n",
      "9200 0.069776885\n",
      "9300 0.09491789\n",
      "9400 0.05810518\n",
      "9500 0.07011444\n",
      "9600 0.16670452\n",
      "9700 0.083131984\n",
      "9800 0.09395922\n",
      "9900 0.053854775\n",
      "10000 0.08073487\n",
      "10100 0.069218144\n",
      "10200 0.06441399\n",
      "10300 0.09414352\n",
      "10400 0.0947795\n",
      "10500 0.0796278\n",
      "10600 0.067096226\n",
      "10700 0.051320378\n",
      "10800 0.10980594\n",
      "10900 0.060865037\n",
      "11000 0.070152126\n",
      "11100 0.051259592\n",
      "11200 0.10865645\n",
      "11300 0.05242078\n",
      "11400 0.078645125\n",
      "11500 0.067101926\n",
      "11600 0.06252171\n",
      "11700 0.092095055\n",
      "11800 0.07453673\n",
      "11900 0.06972007\n",
      "12000 0.059024427\n",
      "12100 0.1140956\n",
      "12200 0.08059464\n",
      "12300 0.10733056\n",
      "12400 0.060916875\n",
      "12500 0.10245228\n",
      "12600 0.06073021\n",
      "12700 0.081636265\n",
      "12800 0.099383906\n",
      "12900 0.100501925\n",
      "13000 0.08648214\n",
      "13100 0.09579462\n",
      "13200 0.067196794\n",
      "13300 0.07075896\n",
      "13400 0.062343925\n",
      "13500 0.052226502\n",
      "13600 0.15921547\n",
      "13700 0.072195895\n",
      "13800 0.13480462\n",
      "13900 0.067282505\n",
      "14000 0.13231714\n",
      "14100 0.11795905\n",
      "14200 0.07103936\n",
      "14300 0.052955575\n",
      "14400 0.064639986\n",
      "14500 0.090335205\n",
      "14600 0.057912447\n",
      "14700 0.09032546\n",
      "14800 0.046707798\n",
      "14900 0.08025354\n",
      "15000 0.10946405\n",
      "15100 0.16341147\n",
      "15200 0.083201915\n",
      "15300 0.030209702\n",
      "15400 0.08566384\n",
      "15500 0.050814446\n",
      "15600 0.08153037\n",
      "15700 0.07364095\n",
      "15800 0.095040746\n",
      "15900 0.052025925\n",
      "16000 0.060406417\n",
      "16100 0.028082358\n",
      "16200 0.07035161\n",
      "16300 0.066183425\n",
      "16400 0.07202591\n",
      "16500 0.04779733\n",
      "16600 0.09921537\n",
      "16700 0.058599614\n",
      "16800 0.08946449\n",
      "16900 0.061203346\n",
      "17000 0.103609964\n",
      "17100 0.07701189\n",
      "17200 0.07963277\n",
      "17300 0.07909684\n",
      "17400 0.08866739\n",
      "17500 0.07647495\n",
      "17600 0.054173768\n",
      "17700 0.07890592\n",
      "17800 0.102853194\n",
      "17900 0.050838355\n",
      "18000 0.07307849\n",
      "18100 0.091047935\n",
      "18200 0.052151605\n",
      "18300 0.12758997\n",
      "18400 0.0595005\n",
      "18500 0.04197135\n",
      "18600 0.09572436\n",
      "18700 0.052471135\n",
      "18800 0.09515394\n",
      "18900 0.06454799\n",
      "19000 0.074542396\n",
      "19100 0.041459836\n",
      "19200 0.064191855\n",
      "19300 0.05834644\n",
      "19400 0.050673064\n",
      "19500 0.08778153\n",
      "19600 0.06605112\n",
      "19700 0.087353416\n",
      "19800 0.10763685\n",
      "19900 0.053969916\n",
      "20000 0.05520149\n",
      "20100 0.05616807\n",
      "20200 0.0772361\n",
      "20300 0.04431587\n",
      "20400 0.08699086\n",
      "20500 0.092330694\n",
      "20600 0.06457241\n",
      "20700 0.05082228\n",
      "20800 0.037383646\n",
      "20900 0.038682222\n",
      "21000 0.09968869\n",
      "21100 0.05105496\n",
      "21200 0.15472496\n",
      "21300 0.042808212\n",
      "21400 0.061639283\n",
      "21500 0.08347489\n",
      "21600 0.10460499\n",
      "21700 0.0737818\n",
      "21800 0.087011375\n",
      "21900 0.047820985\n",
      "22000 0.057250198\n",
      "22100 0.06009965\n",
      "22200 0.03982495\n",
      "22300 0.04864471\n",
      "22400 0.0911047\n",
      "22500 0.065887496\n",
      "22600 0.038113087\n",
      "22700 0.13874377\n",
      "22800 0.06569619\n",
      "22900 0.031005269\n",
      "23000 0.070947915\n",
      "23100 0.079099305\n",
      "23200 0.08297902\n",
      "23300 0.08384282\n",
      "23400 0.07730813\n",
      "23500 0.06892009\n",
      "23600 0.07867388\n",
      "23700 0.050645687\n",
      "23800 0.11506851\n",
      "23900 0.08462826\n",
      "24000 0.17158316\n",
      "24100 0.075073905\n",
      "24200 0.09190382\n",
      "24300 0.112109214\n",
      "24400 0.024314266\n",
      "24500 0.059425652\n",
      "24600 0.059520196\n",
      "24700 0.1376406\n",
      "24800 0.10126916\n",
      "24900 0.085800104\n",
      "25000 0.13358803\n",
      "25100 0.10457748\n",
      "25200 0.096116796\n",
      "25300 0.058653288\n",
      "25400 0.074774265\n",
      "25500 0.08697764\n",
      "25600 0.08529851\n",
      "25700 0.10680789\n",
      "25800 0.065216996\n",
      "25900 0.09308893\n",
      "26000 0.09605256\n",
      "26100 0.04743553\n",
      "26200 0.0722373\n",
      "26300 0.08028252\n",
      "26400 0.10599416\n",
      "26500 0.07284106\n",
      "26600 0.111286685\n",
      "26700 0.13439602\n",
      "26800 0.09026423\n",
      "26900 0.033460885\n",
      "27000 0.06730212\n",
      "27100 0.08826928\n",
      "27200 0.13012159\n",
      "27300 0.071483456\n",
      "27400 0.12662141\n",
      "27500 0.037056226\n",
      "27600 0.088529855\n",
      "27700 0.1409724\n",
      "27800 0.057858404\n",
      "27900 0.0675926\n"
     ]
    }
   ],
   "source": [
    "#training_steps = 5000\n",
    "losses = []\n",
    "\n",
    "for step in tqdm(range(training_steps)):\n",
    "    loss, params, state, sn_state, opt_state = update(params, state, sn_state,\n",
    "                                                      next(rng_seq), opt_state,\n",
    "                                                      next(train))\n",
    "    losses.append(loss)\n",
    "    if step%100==0:\n",
    "        print(step, loss)\n",
    "\n",
    "    if step%5000 ==0:\n",
    "        with open(output_dir+'/model-%d.pckl'%step, 'wb') as file:\n",
    "            pickle.dump([params, state, sn_state], file)\n",
    "\n",
    "with open(output_dir+'/model-final.pckl', 'wb') as file:\n",
    "    pickle.dump([params, state, sn_state], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglog(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from functools import partial\n",
    "#score = partial(model.apply, params, state, next(rng_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res, state = score_fn(batch['x'],batch['s'], is_training=False)\n",
    "_, res, gaussian_score = score_fn(params, state, next(rng_seq), batch)\n",
    "\n",
    "for i in range(10):\n",
    "    ind = i\n",
    "    figure(figsize=(16,4))\n",
    "    subplot(141)\n",
    "    title(\"%0.3f\"%batch['s'][ind,0,0,0])\n",
    "    imshow(batch['x'][ind,...,0],cmap='magma',vmin=-0.05,vmax=0.3)\n",
    "    axis('off')\n",
    "    subplot(142)\n",
    "    imshow(batch['y'][ind,...,0],cmap='magma',vmin=-0.05,vmax=0.3)\n",
    "    axis('off')\n",
    "    subplot(143)\n",
    "    #imshow(res[ind,...,0],cmap='magma')\n",
    "    imshow(res[ind,...,0] + gaussian_score[ind,...,0], cmap='magma')\n",
    "    axis('off')\n",
    "    #title(\"%0.3f\"%std(batch['s'][ind,:,:,0]**2 * res[ind,...,0]))\n",
    "    subplot(144)\n",
    "    #imshow(batch['y'][ind,...,0] + batch['s'][ind,:,:,0]**2 * res[ind,...,0],cmap='magma',vmin=-0.05,vmax=0.3)\n",
    "    imshow(batch['y'][ind,...,0] + batch['s'][ind,:,:,0]**2 * (res[ind,...,0] + gaussian_score[ind,...,0]),cmap='magma',vmin=-0.05,vmax=0.3)\n",
    "    #batch['s'] * (res + gaussian_score))**2\n",
    "    axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
